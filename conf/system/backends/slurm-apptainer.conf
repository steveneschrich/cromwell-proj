backend {
    providers {
        # Per the standard (https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/):
        # cpu
        # memory
        # docker
        # For cluster config, we adopt the standards of biowdl where possible.
        # time_minutes
        slurm-apptainer {
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"                                                               
                      
            config {
                # NOTE: Although tasks will set the 'memory' attribute (as a string with a size), this
                # appears to be converted into memory_mb and memory_gb (float) by this point. So we ignore
                # the memory and use memory_gb as the parameter. This is then truncated to integer with the
                # ceil() function since that's what slurm needs.
                #
                # docker_shell: This variable allows you to specify the shell to use within the docker container. It
                # may vary from container to container which makes it relatively incompatible with the overall backend
                # job_shell. 
                runtime-attributes = """
                    Int time_minutes = 600
                    Int cpu = 1
                    Float? memory_gb = 2
                    String? docker = "ubuntu:latest"
                    String? docker_volumes
                    String? reference_volume
                    String? docker_shell="/bin/bash"
                    Boolean contain_all = true
                """

                submit-docker = """
			        # This preamble is from https://cromwell.readthedocs.io/en/stable/backends/HPC/
			        # It allows us to pull the docker image once and re-use it in subsequent calls.
			        # This does not exactly work, particularly in the case of using ':latest' tags, 
			        # but presumably if one clears out the cache this would work.
        		    CACHE_DIR=$HOME/.apptainer/cache/sif
        		    mkdir -p $CACHE_DIR
        		    LOCK_FILE=$CACHE_DIR/apptainer_pull_flock
        		    DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
        		    IMAGE=$CACHE_DIR/$DOCKER_NAME.sif
        		    (
          			    flock --verbose --exclusive --timeout 900 9 || exit 1
          			    if [ ! -e "$IMAGE" ]; then
            				apptainer build $IMAGE docker://${docker}
          			    fi
        		    ) 9>$LOCK_FILE

                    # Submit the script to SLURM
                    sbatch \
                        -J ${job_name} \
                        -D ${cwd} \
                        -o ${cwd}/execution/stdout \
                        -e ${cwd}/execution/stderr \
                        -t ${time_minutes} \
                        -c ${cpu} \
                        ${"--mem=" + ceil(memory_gb) + "G"} \
                        --wrap "\
                            apptainer exec \
                                ${true="-C" false="" contain_all} \
                                --bind ${cwd}:${docker_cwd} \
                                ${"--bind " + reference_volume} \
                                ${"--bind " + docker_volumes} \
                                $IMAGE \
                                ${docker_shell} \
                                ${docker_script} \
                        "
                """

                # Submit string when there is no "docker" runtime attribute. Per
                # the docs, you can simply exclude the variable. 
                # We could also just assign it to submit-docker directly (if needed).
                #submit = "/usr/bin/env bash ${script}"
                #submit = submit-docker

                # All job control is via slurm
                kill = "scancel ${job_id}"
                kill-docker = "scancel ${job_id}"
                check-alive = "squeue -j ${job_id}"
                job-id-regex = "Submitted batch job (\\d+).*"
                # Since we define check-alive, we can define a polling period once every 2 minutes (for now)
                exit-code-timeout-seconds = 120

                # We define this here as a generic config, but can be overridden
                # in workflow-options default_runtime_attributes.concurrent-job-limit
                concurrent-job-limit = 200


	            # Filesystem configuration
	            # Note: primary thing is to use symlinks instead of hard links. 
		        # Since apptainer (which is this engine) mounts /share, then symlinks
		        # are appropriate.
	            filesystems {
			        local {
				        # localization via soft links in containers
				        # from https://cromwell.readthedocs.io/en/stable/backends/HPC/
				        # Try to soft-link (ln -s), then hard link (ln), and if both fail, then copy the files.
                        localization: [
                        "soft-link", "hard-link", "copy"
                        ]
				        # Since we are preferring soft links for input, we need to have docker
                        # support this. But do note, the source of the links needs to be mounted in
                        # the container! You can use the docker_volumes variable for this purpose as
                        # a default in workflow-options.json.
                        docker.allow-soft-links: true
				
			        }
		        }
            }
        }
    }
}